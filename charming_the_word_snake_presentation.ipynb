{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charming the word snake: Terminology work and language checking with Python\n",
    "\n",
    "_Esther Strauch & Maximilian Rosin, ([parson AG](https://www.parson-europe.com))<br/>\n",
    "tcworld conference 2020_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Python?\n",
    "\n",
    "- Readable and explicit, thus very accessible.\n",
    "- Interpreted language, thus it is easy to run scripts.\n",
    "- A gazillion libraries for almost any recurring tasks.\n",
    "\n",
    "## Terminology extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The text\n",
    "\n",
    "> The COVID-19 pandemic, also known as the coronavirus pandemic, is an ongoing pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The disease was first identified in December 2019 in Wuhan, China. The outbreak was declared a Public Health Emergency of International Concern in January 2020, and a pandemic in March 2020. As of 14 October 2020, more than 38.1 million cases have been confirmed, with more than 1.08 million deaths attributed to COVID-19.\n",
    "\n",
    "Source: [https://en.wikipedia.org/wiki/COVID-19_pandemic]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"The COVID-19 pandemic, also known as the coronavirus pandemic, is an ongoing pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The disease was first identified in December 2019 in Wuhan, China. The outbreak was declared a Public Health Emergency of International Concern in January 2020, and a pandemic in March 2020. As of 14 October 2020, more than 38.1 million cases have been confirmed, with more than 1.08 million deaths attributed to COVID-19.\"\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the spacy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a model of the English language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding the text into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### A little helper function for pretty-printing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def print_table(data,header):\n",
    "    print(tabulate(data, headers=header, tablefmt=\"simple\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory: token, lemma, part-of-speech\n",
    "\n",
    "> The dog has a wet nose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc = nlp(\"The dog has a wet nose.\")\n",
    "\n",
    "sample_sentence = []\n",
    "\n",
    "for token in sample_doc:\n",
    "    sample_sentence.append([token.text,token.lemma_,token.pos_])\n",
    "\n",
    "print_table(sample_sentence,[\"Token\",\"Lemma\",\"POS\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering for relevant terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relevant(token):\n",
    "    pos_tag_concepts = [\"NOUN\",\"PROPN\",\"VERB\"]\n",
    "    if token.pos_ in pos_tag_concepts:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the initial list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_word_terms = []\n",
    "\n",
    "for token in doc:\n",
    "    if is_relevant(token):\n",
    "        one_word_terms.append([token.lemma_ , token.pos_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_word_terms = sorted(one_word_terms)\n",
    "one_word_terms_no_dups = [one_word_terms[i] for i in range(len(one_word_terms)) \n",
    "                           if i == 0 or one_word_terms[i] != one_word_terms[i-1]]\n",
    "one_word_terms = one_word_terms_no_dups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_table(one_word_terms,[\"Term\",\"POS\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spacy's sentenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a list of sentences and their lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    terms = []\n",
    "    for token in sent.subtree:\n",
    "        if is_relevant(token):\n",
    "            terms.append([token.lemma_ , token.pos_])\n",
    "    terms = sorted(terms)\n",
    "    terms = [terms[i] for i in range(len(terms)) if i == 0 or terms[i] != terms[i-1]]\n",
    "    sentences.append({\"sentence\": sent.text,\n",
    "                    \"terms\": terms})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    print(\"Sentence: \" + sentence[\"sentence\"] + \"\\n\")\n",
    "    print_table(sentence[\"terms\"],[\"Term\",\"POS\"])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the sample sentences to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for one_word_term in one_word_terms:\n",
    "    for sentence in sentences:\n",
    "        if one_word_term in sentence[\"terms\"]:\n",
    "            one_word_term.append(sentence[\"sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the results again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in one_word_terms:\n",
    "    print(\"Term: \" + term[0])\n",
    "    print(\"POS: \" + term[1])\n",
    "    print(\"Sentence: \" + term[2] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding definitions (where possible)\n",
    "\n",
    "Data source: [Free Wordset Dictionary](https://github.com/wordset/wordset-dictionary/tree/master/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "postag_map = {\"NOUN\" : \"noun\",\n",
    "              \"PROPN\": \"noun\",\n",
    "              \"VERB\" : \"verb\"}\n",
    "\n",
    "for term in one_word_terms:\n",
    "    dictionary_name = \"wordset-dictionary/\" + term[0][0] + \".json\"\n",
    "    try:\n",
    "        with open(dictionary_name,\"r\", encoding=\"utf-8\") as dictionary_file:\n",
    "            dictionary_data = json.load(dictionary_file)\n",
    "            try:\n",
    "                definitions = \"\"\n",
    "                for meaning in dictionary_data[term[0]][\"meanings\"]:\n",
    "                    if meaning[\"speech_part\"] == postag_map[term[1]]:definitions += (meaning[\"def\"] + \",\")\n",
    "                term.append(definitions)\n",
    "            except KeyError:\n",
    "                term.append(\"~definition missing~\")\n",
    "                print(\"Term \" + term[0] + \" not found in dictionary.\")\n",
    "    except FileNotFoundError:\n",
    "        term.append(\"~definition missing~\")\n",
    "        print(\"No matching dictionary found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in one_word_terms:\n",
    "    print(\"Term: \" + term[0])\n",
    "    print(\"POS: \" + term[1])\n",
    "    print(\"Sentence: \" + term[2])\n",
    "    print(\"Definitions: \" + term[3] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the list to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"one-word-terms.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for term in one_word_terms:\n",
    "        csvwriter.writerow(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further ideas\n",
    "\n",
    "- Count the frequency of terms.\n",
    "- Add **two-word terms** by searching for bigrams. This is a little bit tougher than it sounds.\n",
    "- Evaluate relations between terms by looking for **colocations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking writing rules\n",
    "\n",
    "### The gruesome sample text\n",
    "\n",
    "> Der folgende Text soll helfen, Schreibregeltests zu veranschaulichen. Dieses phantasmagorische Ungetüm von einem Satz ist zum Beispiel fast schon lächerlich lang und trotzdem erlaubt es mir die deutsche Sprache, dass ich ihn mit voller Absicht auf diese überaus erstaunliche Überlänge bringen kann. Hier ein kurzer Satz. Und noch einer. Als nächstes ein Satz mit einem sehr langen Wort. Denn wer kennt es nicht, das berühmte mecklenburg-vorpommerische Rindfleischetikettierungsüberwachungsaufgabenübertragungsgesetz. Die Erfindung solcher Worte erfolgt meist durch Beamte, damit die Kommunikation mit den Bürgern einer Eindeutigkeit genügt. Oder besser, Beamte erfinden oft lange Worte. Aber auch vor eingeschobenen Nebensätzen, die in der Mitte eines Satzes eingeschoben werden, sei gewarnt. Aufzählungen von Sachverhalten, Worten, Begriffen, Fakten oder Nebensächlichkeiten verlagert man besser in separate Listen.\n",
    "\n",
    "### Loading text into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gruesome_text = \"Der folgende Text soll helfen, Schreibregeltests zu veranschaulichen. Dieses phantasmagorische Ungetüm von einem Satz ist zum Beispiel fast schon lächerlich lang und trotzdem erlaubt es mir die deutsche Sprache, dass ich ihn mit voller Absicht auf diese überaus erstaunliche Überlänge bringen kann. Hier ein kurzer Satz. Und noch einer. Als nächstes ein Satz mit einem sehr langen Wort. Denn wer kennt es nicht, das berühmte mecklenburg-vorpommerische Rindfleischetikettierungsüberwachungsaufgabenübertragungsgesetz. Die Erfindung solcher Worte erfolgt meist durch Beamte, damit die Kommunikation mit den Bürgern einer Eindeutigkeit genügt. Oder besser, Beamte erfinden oft lange Worte. Aber auch vor eingeschobenen Nebensätzen, die in der Mitte eines Satzes eingeschoben werden, sei gewarnt. Aufzählungen von Sachverhalten, Worten, Begriffen, Fakten oder Nebensächlichkeiten verlagert man besser in separate Listen.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the language model (this time a German one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_de = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding the text into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_de(gruesome_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding forbidden words\n",
    "\n",
    "#### List of forbidden words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forbidden_words = [\"erfolgen\",\"fast\",\"Sachverhalt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking every token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doc.sents:\n",
    "    for token in sentence.subtree:\n",
    "        if token.lemma_ in forbidden_words:\n",
    "            print(\"Forbidden word: \"\n",
    "                  + \"'\" + token.lemma_ + \"'\"\n",
    "                  + \" in '\" + sentence.text + \"'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding long words\n",
    "\n",
    "#### Counting syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = [\"a\",\"e\",\"i\",\"o\",\"u\",\"ä\",\"ü\",\"ö\",\"y\"]\n",
    "diphtongs = [\"aa\",\"ai\",\"au\",\"ay\",\"ee\",\"ei\",\"eu\",\"ey\",\"ie\",\"io\",\"oa\",\"oi\",\"oo\",\"oy\",\"ui\",\"ya\",\"ye\",\"yi\",\"yo\",\"yu\"]\n",
    "\n",
    "def count_occurences(text,substrings):\n",
    "    occurences = 0\n",
    "    for substring in substrings:\n",
    "        occurences += text.count(substring)\n",
    "    return occurences\n",
    "\n",
    "def count_syllables(text):\n",
    "    num_vowels = count_occurences(text,vowels)\n",
    "    num_diphtongs = count_occurences(text,diphtongs)\n",
    "    return (num_vowels - num_diphtongs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking every token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doc.sents:\n",
    "    for token in sentence.subtree:\n",
    "        syllables = count_syllables(token.text)\n",
    "        if (syllables > 3) or (len(token.text) > 10):\n",
    "            print(\"Long word \"\n",
    "                  + \"(\" + str(syllables)  + \" syllables, \"\n",
    "                  + str((len(token.text))) + \" characters\" + \"): \"\n",
    "                  + \"'\" + token.text + \"'\"\n",
    "                  + \" in '\" + sentence.text + \"'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding long sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sentence in doc.sents:\n",
    "    words = [ token.text for token in sentence.subtree if token.pos_ not in [\"PUNCT\",\"SYM\",\"X\"] ]\n",
    "    if len(words) > 15:\n",
    "        print(\"Long sentence \" + \"(\" + str(len(words)) + \" words): \"\n",
    "              + sentence.text +\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependent clauses, enumerations...: What commas can tell you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doc.sents:\n",
    "    num_of_commas = count_occurences(sentence.text,\",\")\n",
    "    if num_of_commas > 1:\n",
    "        print(\"Multiple commas \" + \"(\" + str(num_of_commas) + \" commas): \"\n",
    "                  + \"'\" + sentence.text + \"'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding nominalizations.. or at least the worst ones\n",
    "\n",
    "#### Identifying nominalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominalization_hints = [\"ung\",\"keit\",\"heit\",\"tion\"]\n",
    "\n",
    "def is_nominalization(word,pos):\n",
    "    for nominalization_hint in nominalization_hints:\n",
    "        if word.endswith(nominalization_hint) and pos in [\"NOUN\",\"PROPN\"]:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking every sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doc.sents:\n",
    "    for token in sentence.subtree:\n",
    "        if is_nominalization(token.text,token.pos_):\n",
    "            print(\"Possible nominalization: \"\n",
    "                  + \"'\" + token.lemma_ + \"'\"\n",
    "                  + \" in '\" + sentence.text + \"'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further ideas\n",
    "\n",
    "- Finding passive sentences.\n",
    "- Checking for \"dass\" vs. \"das\".\n",
    "- Calculating readibility indeces.\n",
    "- Estimating reading time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
